<html>
  <head>
    <!-- Load TensorFlow.js -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <!-- Load Posenet -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>
    <script src="https://canvasjs.com/assets/script/canvasjs.min.js"></script>
 </head>

  <body style="background-color: black;height: 1200px;">
    <input type="text" id="ytlink" value="">
    Average Error <div id="error"></div>
    <button onclick="onStart()">Start Pose Match</button>
    <div class="row margin">
        <div class="col-md-6">
          <video id="inputVideo" autoplay muted playsinline></video>
        </div>
        <div class="col-md-6">
          <canvas id="overlay" />
        </div>
    </div>
    <video id="ytvideo" width="500" height="400" src="" autoplay crossorigin='anonymous' playsinline controls></video>
    <div id="chartContainer" class="my-5"></div>
  </body>
  <!-- Place your code in the script tag below. You can also use an external .js file -->
  <script>
    var chart;
    chart = new CanvasJS.Chart("chartContainer", {
      animationEnabled: true,
      theme: "light2",
      title:{
        text: "Average Error in pose"
      },
      data: [{        
		    type: "line",
      	indexLabelFontSize: 16,
        dataPoints:[] 
      }]
    });
    chart.render();
    var flipHorizontal = false;
    var net = null;
    var isFaceDetectionModelLoaded = false;

    function onStart() {
      console.log("start");
      document.getElementById("ytvideo").src = document.getElementById("ytlink").value;
      onPlay()
    }

    async function onPlay() {
        const videoEl1 = document.getElementById('inputVideo')
        const videoEl2 = document.getElementById('ytvideo')

        if(!videoEl1 || !videoEl2 || videoEl1.paused || videoEl1.ended || videoEl2.paused || videoEl2.ended || !isFaceDetectionModelLoaded)
        return setTimeout(() => onPlay( ))

        const pose1 = await net.estimateSinglePose(videoEl1, {
            flipHorizontal: true
        });
        const pose2 = await net.estimateSinglePose(videoEl2, {
            flipHorizontal: true
        });

        if(pose1 && pose2) {
          let err = poseMatch(pose1,pose2)
          document.getElementById("error").textContent = err
          chart.options.data[0].dataPoints.push({y:err})
          chart.render();
        }

        setTimeout(() => onPlay())
    }

    async function run() {
        // load face detection and face expression recognition models
        net = await posenet.load()
        isFaceDetectionModelLoaded= true;
        // try to access users webcam and stream the images
        // to the video element
        const stream = await navigator.mediaDevices.getUserMedia({ video: {} })
        const videoEl = document.getElementById('inputVideo')
        videoEl.srcObject = stream
    }

    function poseMatch(pose1,pose2) {
      let sum = 0;
      for(let i=0; i<17; i++) {
        sum += ((pose1.keypoints[i].position.x*pose1.keypoints[i].score-pose2.keypoints[i].position.x*pose2.keypoints[i].score)**2 + (pose1.keypoints[i].position.y*pose1.keypoints[i].score-pose2.keypoints[i].position.y*pose2.keypoints[i].score)**2)**0.5
      }
      return (sum/17);
    }

    function updateResults() {}

    window.onload = run;
  </script>
</html>